# -*- coding: utf-8 -*-
"""Copy of Getting Started with PyTorch on Cloud TPUs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cjqwlcFMeYCkz4pmykSWKNJyvFD78xqb

## Getting Started with PyTorch on Cloud TPUs

This notebook will show you how to:

* Install PyTorch/XLA on Colab, which lets you use PyTorch with TPUs.
* Run basic PyTorch functions on TPUs, like creating and adding tensors.
* Run PyTorch modules and autograd on TPUs.
* Run PyTorch networks on TPUs.

PyTorch/XLA is a package that lets PyTorch connect to Cloud TPUs and use TPU cores as devices. Colab provides a free Cloud TPU system (a remote CPU host + four TPU chips with two cores each) and installing PyTorch/XLA only takes a couple minutes. 

Even though Colab offers eight TPU cores, this notebook only uses one for simplicity. More information about running PyTorch on TPUs can be found on [PyTorch.org](http://pytorch.org/xla/), including how to run PyTorch networks on multiple TPU cores simultaneously. Other Colab notebooks also show how to use multiple TPU cores, including [this one](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb#scrollTo=Afwo4H7kSd8P) which trains a network on the MNIST dataset and [this one](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet18-training.ipynb#scrollTo=_2nL4HmloEyl) which trains a ResNet18 architecture on CIFAR10. 

These and other Colab notebooks, as well as Google Cloud Platform (GCP) tutorials, can be found [here](https://github.com/pytorch/xla/tree/master/contrib/colab). Check out our [NeurIPS 2019 Fast Neural Style Transfer demo](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/style_transfer_inference.ipynb#scrollTo=EozMXwIV9iOJ), where you can apply different styles (filters) to your own images!

To use PyTorch on Cloud TPUs in your own Colab notebook you can copy this one, or copy the setup cell below and configure your Colab environment to use TPUs. 

Finally, this notebook is intended for people already familiar with PyTorch, a popular open-source deep learning framework. If you haven't used PyTorch before you might want to review the tutorials at https://pytorch.org/ before continuing.

<h3>  &nbsp;&nbsp;Use Colab Cloud TPU&nbsp;&nbsp; <a href="https://cloud.google.com/tpu/"><img valign="middle" src="https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png" width="50"></a></h3>

* On the main menu, click Runtime and select **Change runtime type**. Set "TPU" as the hardware accelerator.
* The cell below makes sure you have access to a TPU on Colab.
"""

#import os
#assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'

"""## Installing PyTorch/XLA

Run the following cell (or copy it into your own notebook!) to install PyTorch, Torchvision, and PyTorch/XLA. It will take a couple minutes to run.

The PyTorch/XLA package lets PyTorch connect to Cloud TPUs. (It's named PyTorch/XLA, not PyTorch/TPU, because XLA is the name of the TPU compiler.) In particular, PyTorch/XLA makes TPU cores available as PyTorch devices. This lets PyTorch create and manipulate tensors on TPUs.
"""

VERSION = "20200325"  #@param ["1.5" , "20200325", "nightly"]

"""## Creating and Manipulating Tensors on TPUs

PyTorch uses Cloud TPUs just like it uses CPU or CUDA devices, as the next few cells will show. Each core of a Cloud TPU is treated as a different PyTorch  device.
"""

# imports pytorch
#import torch

# imports the torch_xla package
#import torch_xla
#import torch_xla.core.xla_model as xm

from skimage.transform import resize
import json, os, pickle, matplotlib.pyplot as plt, numpy as np, pandas as pd
import torch, torchvision, torch.nn as nn, torch.optim as optim

from tqdm import tqdm_notebook as tqdm
from random import randint
from sklearn.model_selection import train_test_split
from imageio import imread
from collections import Counter
from imageio import imread
from PIL import Image
from torch.utils.data import TensorDataset, Dataset, DataLoader
from torch.utils.data._utils.collate import default_collate
from torchvision import models, transforms
from sklearn.metrics import roc_auc_score
from math import floor, sqrt
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import atexit
import traceback
@atexit.register
def goodbye():
    traceback.print_exc()


chexpert = pd.read_csv('mimic-cxr-2.0.0-chexpert.csv', index_col='study_id')

###############

split = pd.read_csv('mimic-cxr-2.0.0-split.csv', index_col='study_id')

negbio = pd.read_csv('mimic-cxr-2.0.0-negbio.csv', index_col='study_id')

metadata = pd.read_csv('mimic-cxr-2.0.0-metadata.csv', index_col='study_id')

recordlist = pd.read_csv('cxr-record-list.csv')
studylist = pd.read_csv('cxr-study-list.csv')

paOnly = metadata[metadata['ViewPosition']=='AP']
paOnlyRL = paOnly.set_index('dicom_id').join(recordlist.set_index('dicom_id'), lsuffix='', rsuffix='_rl')
rlsplit = paOnlyRL.join(split.set_index('dicom_id'), lsuffix='',rsuffix='_split')
rlsplit = rlsplit.set_index('study_id').join(chexpert, lsuffix='_l', rsuffix='_r')
pn = rlsplit[rlsplit['Pneumothorax'].notnull()]

rlsplit=pn

print(split['split'].value_counts())
print(rlsplit['split'].value_counts())
print(rlsplit['Pneumothorax'].value_counts())

trainset = rlsplit[rlsplit['split']=='train']
testset = rlsplit[rlsplit['split']=='test']
validset = rlsplit[rlsplit['split']=='validate']

import os
import torch
import torchvision.transforms as transforms
import torchvision.datasets as dset
import torch.utils.data as data

normalize_transform = transforms.Compose([transforms.ToTensor(),
                                              transforms.Normalize((0.485, 0.456, 0.406),
                                                                   (0.229, 0.224, 0.225))])
#normalize_transform = transforms.Compose([transforms.ToTensor(),
#                                              transforms.Normalize((0.485),
#                                                                   (0.229))])
train_large_transform = transforms.Compose([transforms.RandomResizedCrop(224),
                                                transforms.RandomHorizontalFlip()])
val_large_transform = transforms.Compose([transforms.Resize(256),
                                              transforms.CenterCrop(224)])
train_small_transform = transforms.Compose([transforms.Pad(4),
                                                transforms.RandomCrop(32),
                                                transforms.RandomHorizontalFlip()])

train_transform = transforms.Compose([train_large_transform, normalize_transform])
val_transform = transforms.Compose([val_large_transform, normalize_transform])

import numpy as np

class mimicDataset(torch.utils.data.Dataset):

    def __init__(self, pdframe, root_dir, transform):
        """
        Args:
            text_file(string): path to text file
            root_dir(string): directory with all train images
        """
        self.pdframe = pdframe
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.pdframe)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.pdframe.iloc[idx, 11]).replace('.dcm','.jpg')
        image = Image.open(img_name).convert('RGB')
        image = self.transform(image)
        label_raw = self.pdframe.iloc[idx, 27] #.astype(np.float64)
        
        if False: # np.isnan(label_raw):
          label = np.int64(3)
        elif label_raw < 0:
          label = np.int64(2)
        #elif label == 0.:
        #  label = 0
        else:
          label = label_raw.astype(np.int64)
          #print('label', label)
          #assert(False)
        #assert(label>=0 and label < 4)
        return image, label

import sys
sys.path.insert(1, './L2T-ww')

trainset.head()

#prepare dataloader
batchSize = 32 # 128

pdDatasets = [trainset, testset, validset]
pdsts = [
         torch.utils.data.DataLoader(mimicDataset(trainset, root_dir='./data', transform=train_transform), batch_size=batchSize, shuffle=True, num_workers=0),
         torch.utils.data.DataLoader(mimicDataset(testset, root_dir='./data', transform=train_transform), batch_size=batchSize, shuffle=True, num_workers=0),
         torch.utils.data.DataLoader(mimicDataset(validset, root_dir='./data', transform=val_transform), batch_size=batchSize, shuffle=True, num_workers=0),
        ]

import importlib
import train_l2t_ww

import os
os.environ['CUDA_LAUNCH_BLOCKING']='1'

train_l2t_ww.main(mimicLoader=pdsts, arguments=['--num-classes', '3', '--dataroot', 'data']) # , given_dev=xm.xla_device())
sys.exit(0)


"""As mentioned above, the PyTorch/XLA package (torch_xla) lets PyTorch use TPU devices. The `xla_device()` function returns the TPU's "default" core as a device. This lets PyTorch creates tensors on TPUs:"""

# Creates a random tensor on xla:1 (a Cloud TPU core)
dev = xm.xla_device()
t1 = torch.ones(3, 3, device = dev)
print(t1)

"""See the documentation at http://pytorch.org/xla/ for a description of all public PyTorch/XLA functions. Here `xm.xla_device()` acquired the first Cloud TPU core ('xla:1'). Other cores can be directly acquired, too:"""

# Creating a tensor on the second Cloud TPU core
second_dev = xm.xla_device(n=2, devkind='TPU')
t2 = torch.zeros(3, 3, device = second_dev)
print(t2)

"""It is recommended that you use functions like `xm.xla_device()` over directly specifying TPU cores. A future Colab tutorial will show how to easily train a network using multiple cores (or you can look at [an example](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb#scrollTo=Afwo4H7kSd8Phttps://)).

Tensors on TPUs can be manipulated like any other PyTorch tensor. The following cell adds, multiplies, and matrix multiplies two tensors on a TPU core:
"""

a = torch.randn(2, 2, device = dev)
b = torch.randn(2, 2, device = dev)
print(a + b)
print(b * 2)
print(torch.matmul(a, b))

"""This next cell runs a 1D convolution on a TPU core:"""

# Creates random filters and inputs to a 1D convolution
filters = torch.randn(33, 16, 3, device = dev)
inputs = torch.randn(20, 16, 50, device = dev)
torch.nn.functional.conv1d(inputs, filters)

"""And tensors can be transferred between CPU and TPU. In the following cell, a tensor on the CPU is copied to a TPU core, and then copied back to the CPU again. Note that PyTorch makes copies of tensors when transferring them across devices, so `t_cpu` and `t_cpu_again` are different tensors."""

# Creates a tensor on the CPU (device='cpu' is unnecessary and only added for clarity)
t_cpu = torch.randn(2, 2, device='cpu')
print(t_cpu)

t_tpu = t_cpu.to(dev)
print(t_tpu)

t_cpu_again = t_tpu.to('cpu')
print(t_cpu_again)

"""## Running PyTorch modules and autograd on TPUs

Modules and autograd are fundamental PyTorch components. 

In PyTorch, every stateful function is a module. Modules are Python classes augmented with metadata that lets PyTorch understand how to use them in a neural network. For example, linear layers are modules, as are entire networks. Since modules are stateful, they can be placed on devices, too. PyTorch/XLA lets us place them on TPU cores:
"""

# Creates a linear module
fc = torch.nn.Linear(5, 2, bias=True)

# Copies the module to the XLA device (the first Cloud TPU core)
fc = fc.to(dev)

# Creates a random feature tensor
features = torch.randn(3, 5, device=dev, requires_grad=True)

# Runs and prints the module
output = fc(features)
print(output)

"""Autograd is the system PyTorch uses to populate the gradients of weights in a neural network. See [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) for details about PyTorch's autograd. When a module is run on a TPU core, its gradients are also populated on the same TPU core by autograd. The following cell demonstrates this:"""

output.backward(torch.ones_like(output))
print(fc.weight.grad)

"""## Running PyTorch networks on TPUs

As mentioned above, PyTorch networks are also modules, and so they're run in the same way. The following cell runs a relatively simple PyTorch network from the [PyTorch tutorial docs](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) on a TPU core:
"""

import torch.nn as nn
import torch.nn.functional as F

# Simple example network from 
# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


# Places network on the default TPU core
net = Net().to(dev)

# Creates random input on the default TPU core
input = torch.randn(1, 1, 32, 32, device=dev)

# Runs network
out = net(input)
print(out)

"""As in the previous snippets, running PyTorch on a TPU just requires specifying a TPU core as a device.

## More PyTorch on TPUs!

This "Getting Started" notebook showed you how to:

* Install PyTorch/XLA on Colab, which lets you use PyTorch with TPUs.
* Run basic PyTorch functions on TPUs, like creating and adding tensors.
* Run PyTorch modules and autograd on TPUs.
* Run PyTorch networks on TPUs.

Working with tensors, running modules, and running entire networks on a Cloud TPU is as simple as installing PyTorch/XLA and telling PyTorch to use the Colab TPU as its device.

You're encouraged to try PyTorch/XLA on Colab and GCP, too! You can copy this notebook to get started, or just copy the PyTorch/XLA setup cell above. The PyTorch/XLA team would love to hear your ideas/suggestions/comments, too! For now, the best way to contact the team is with an issue on our Github: https://github.com/pytorch/xla/issues. 

Future tutorials will cover:

* Training a network using multiple TPU cores.
* Loading batched data onto TPUs.

You can already see examples of training a network on multiple TPU cores and loading batched data onto TPUs [here](https://github.com/pytorch/xla/tree/master/contrib/colab).
"""
